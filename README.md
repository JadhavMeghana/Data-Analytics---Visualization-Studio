# Enterprise Sales Analytics & Visualization System

## ğŸ“‹ Business Problem

This system addresses the need for automated, enterprise-grade sales data analytics in an IT services environment. It processes large volumes of sales transaction data, validates data quality, calculates key performance indicators (KPIs), and generates automated reports and visualizations.

**Key Business Challenges Solved:**
- Manual data analysis is time-consuming and error-prone
- Lack of real-time insights into sales performance
- Inconsistent KPI calculations across teams
- No automated anomaly detection
- Limited visibility into regional and customer performance

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV/Excel  â”‚
â”‚   Files     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Loader   â”‚â”€â”€â”€â”€â”€â–¶â”‚   Oracle     â”‚
â”‚  (Data Ingestion)â”‚      â”‚  Database    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  PL/SQL Procedures     â”‚
                    â”‚  - Data Validation     â”‚
                    â”‚  - KPI Calculations    â”‚
                    â”‚  - Error Logging       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Python Analytics     â”‚
                    â”‚  - Insight Generation â”‚
                    â”‚  - Chart Creation     â”‚
                    â”‚  - Report Generation  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Reports & Charts      â”‚
                    â”‚  - PNG Charts         â”‚
                    â”‚  - HTML Dashboards   â”‚
                    â”‚  - CSV Summaries     â”‚
                    â”‚  - Text Reports      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

- **Database**: Oracle SQL + PL/SQL (Procedures, Packages)
- **Backend**: Python 3.8+
  - pandas: Data manipulation
  - cx_Oracle: Oracle database connectivity
  - matplotlib/seaborn/plotly: Visualization
  - PyYAML: Configuration management
- **Automation**: Unix Shell Scripting + Cron
- **Optional UI**: Flask (for simple web interface)

## ğŸ“Š KPIs Delivered

1. **Revenue by Region**: Total sales revenue segmented by geographic region
2. **Monthly Revenue Trend**: Time-series analysis of revenue over months
3. **Top Customers**: Identification of highest-value customers
4. **Product Performance**: Revenue analysis by product
5. **Average Transaction Value**: Mean value of individual transactions

## ğŸ¤– Automation Achieved

- **Daily Data Processing**: Automated CSV/Excel file ingestion
- **Data Validation**: Automated quality checks and error logging
- **KPI Calculation**: Scheduled computation of all KPIs
- **Report Generation**: Automated creation of charts and summaries
- **Anomaly Detection**: Rule-based outlier identification
- **Trend Analysis**: Automated trend detection and insights

## ğŸ“ Project Structure

```
enterprise-sales-analytics/
â”œâ”€â”€ database/          # Oracle SQL/PL/SQL scripts
â”œâ”€â”€ python/            # Python analytics modules
â”œâ”€â”€ scripts/           # Unix shell scripts
â”œâ”€â”€ data/              # Input/archive data files
â”œâ”€â”€ reports/           # Generated reports and charts
â”œâ”€â”€ logs/              # Application logs
â””â”€â”€ config.yaml        # Configuration file
```

## ğŸš€ How to Run Locally

### Prerequisites

1. **Oracle Database** (Oracle Express Edition or higher)
   - Create user: `SALES_ANALYTICS`
   - Grant necessary privileges

2. **Python 3.8+** with pip

3. **Unix-like environment** (Linux/Mac/WSL for Windows)

### Installation Steps

1. **Clone/Download the project**
   ```bash
   cd enterprise-sales-analytics
   ```

2. **Create Python virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure database**
   - Copy `config.yaml.example` to `config.yaml`
   - Update database credentials in `config.yaml`

5. **Set up Oracle database**
   ```bash
   sqlplus SALES_ANALYTICS/password@localhost:1521/XE @database/scripts/install_all.sql
   ```

6. **Prepare your data (or use sample links below)**
   - Place your CSV/XLSX files in `data/input/` if using the pipeline scripts
   - For the Streamlit app, just upload the files directly (multiple files supported)
   - Optional legacy sales schema for pipeline: transaction_date, customer_id, product_id, quantity, unit_price, total_amount, region_id

### Running the Pipeline

**Option 1: Run with CSV file**
```bash
./scripts/run_pipeline.sh data/input/sales_data.csv
```

**Option 2: Run without data load (KPI calculation only)**
```bash
./scripts/run_pipeline.sh
```

**Option 3: Run individual components**
```bash
# Validate data
./scripts/validate_data.sh

# Generate reports
./scripts/generate_reports.sh
```

**Option 4: Run Python directly**
```bash
python python/main.py data/input/sales_data.csv
```

### Streamlit Analytics Studio (dynamic schema, your own data)

```bash
# Activate venv
.\venv\Scripts\Activate.ps1   # Windows
source venv/bin/activate      # macOS/Linux

# Install streamlit if needed
pip install streamlit

# Run Streamlit app
streamlit run python/streamlit_app.py --server.maxUploadSize=500
```

Features:
- Upload your own CSV/XLSX (multiple files supported); no bundled sample fallback
- Dynamic column mapping (pick date, numeric, and category columns)
- Generic KPIs and visuals; sales-specific charts appear only if classic sales columns exist
- Outlier detection (requires a chosen date + numeric column)
- Dataset narrative summary and data preview

### Sample datasets for testing (XLSX)
- Product Sales by Region: https://excelx.com/wp-content/uploads/2025/06/Product-Sales-Region.xlsx
- Online Store Orders: https://excelx.com/wp-content/uploads/2025/06/Online-Store-Orders.xlsx
- Retail Store Transactions: https://excelx.com/wp-content/uploads/2025/06/Retail-Store-Transactions.xlsx

### Setting Up Cron Job

```bash
# Make scripts executable
chmod +x scripts/*.sh

# Setup cron (runs daily at 2 AM)
./scripts/setup_cron.sh
# Follow instructions to add cron job
```

## ğŸ“ˆ Sample CSV Format

```csv
transaction_date,customer_id,product_id,quantity,unit_price,total_amount,region_id
2024-01-15,1,101,10,50.00,500.00,1
2024-01-16,2,102,5,100.00,500.00,2
2024-01-17,1,103,20,25.00,500.00,1
```

## ğŸ“ Output Files

- **Charts**: `reports/charts/`
  - `revenue_by_region.png`
  - `monthly_revenue_trend.png`
  - `top_customers.png`
  - `interactive_dashboard.html`

- **Reports**: `reports/summaries/`
  - `kpi_summary_YYYYMMDD.csv`
  - `report_YYYYMMDD_HHMMSS.txt`

- **Insights**: `reports/insights/`
  - `insights_YYYYMMDD_HHMMSS.txt`

- **Logs**: `logs/`
  - `app.log`
  - `error.log`
  - `pipeline_YYYYMMDD_HHMMSS.log`

## ğŸ” Key Features

- âœ… **Data Validation**: Comprehensive checks for data quality
- âœ… **Error Handling**: Robust error logging and recovery
- âœ… **Performance**: Indexed database tables for fast queries
- âœ… **Scalability**: Handles large datasets efficiently
- âœ… **Automation**: Fully automated pipeline execution
- âœ… **Visualization**: Multiple chart types (static and interactive)
- âœ… **Insights**: Rule-based anomaly and trend detection

## ğŸ‘¨â€ğŸ’¼ Interview-Ready Features

This project demonstrates:
- Enterprise database design (normalized schema, indexes, constraints)
- PL/SQL programming (packages, procedures, error handling)
- Python data engineering (pandas, database connectivity)
- Unix shell scripting and automation
- Data validation and quality assurance
- KPI calculation and business intelligence
- Visualization and reporting
- Production-ready code structure

## ğŸ“ Support

For issues or questions, check the logs in `logs/` directory.

## ğŸ“„ License

This project is created for educational and interview preparation purposes.

